{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe33fa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from top2vec import Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3839797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "country = \"United Kingdom\"\n",
    "camp = 'support'\n",
    "sentiment ='positive'\n",
    "\n",
    "fileName = 'support_campaign_' + country\n",
    "path = ('campaign_category_results/campaign_category_results/support_campaign/' + fileName + \".csv\")\n",
    "\n",
    "data = pd.read_csv(path)\n",
    "data_text = data[['Cleaned_Tweet','Tweet_lemmatized','sentiment']]\n",
    "data_text['ta_tweet'] = data_text['Tweet_lemmatized'].apply(lambda x: x[1:-1])\n",
    "data_text['ta_tweet'] = data_text['ta_tweet'].str.replace(r\"[\\\"\\',]\", '',regex=True)\n",
    "\n",
    "\n",
    "documents = data_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b75dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = documents[documents['sentiment'] >= 4].copy(deep=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581db2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = documents[documents['sentiment'] <= 2].copy(deep=True)\n",
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77251e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = documents[documents['sentiment'] == 3].copy(deep=True)\n",
    "# df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3989fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(documents) == (len(df1) + len(df2) + len(df3))):\n",
    "    print('True')\n",
    "else:\n",
    "    print('False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86108ea7-5e79-4340-a756-c36f66550f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = documents.ta_tweet.tolist()\n",
    "# docs\n",
    "positive_docs = df1.ta_tweet.tolist()\n",
    "negative_docs = df2.ta_tweet.tolist()\n",
    "neutral_docs = df3.ta_tweet.tolist()\n",
    "\n",
    "positive_docs\n",
    "# neutral_docs\n",
    "# negative_docs\n",
    "# print (Top2Vec.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8d76a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Top2Vec(positive_docs, speed=\"deep-learn\", ngram_vocab=\"true\", embedding_model=\"doc2vec\",split_documents=\"true\",verbose=\"true\",workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb931e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b52979b-1a93-46f5-8bae-53da85c744c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = []\n",
    "bigrams = []\n",
    "trigrams = []\n",
    "other = []\n",
    "for word in model.vocab:\n",
    "    size = len(word.split())\n",
    "    if size == 1:\n",
    "        unigrams.append(word)\n",
    "    elif size == 2:\n",
    "        bigrams.append(word)\n",
    "    elif size == 3:\n",
    "        trigrams.append(worod)\n",
    "    else:\n",
    "        other.append(word)\n",
    "print (len(unigrams))\n",
    "print (len(bigrams))\n",
    "print (len(trigrams))\n",
    "print (len(other))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53841d09-80b5-44fc-97c6-709d76e5ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81525717-55f7-4124-ad06-8310424aaa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bb76a9-2f59-413c-8e6a-c632f957e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, document_scores, document_ids = model.search_documents_by_topic(topic_num=2, num_docs=5)\n",
    "for doc, score, doc_id in zip(documents, document_scores, document_ids):\n",
    "    print(f\"Document: {doc_id}, Score: {score}\")\n",
    "    print(\"-----------\")\n",
    "    print(doc)\n",
    "    print(\"-----------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d0c53-2f14-4b61-9773-b4bd4f95fdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of topics : \" + str(model.get_num_topics()))\n",
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "print (topic_sizes)\n",
    "print (topic_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e16cd97-5792-4cd9-8361-726a9636f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_nums = model.get_topics(reduced=True)\n",
    "\n",
    "topic_keys=pd.DataFrame()\n",
    "\n",
    "topic_keys[\"Topic\"] =\"\"\n",
    "topic_keys[\"Count\"] =\"\"\n",
    "topic_keys[\"TopicLabel\"] =\"\"\n",
    "topic_keys[\"WordScore\"] = \"\"\n",
    "topic_keys[\"Sentiment\"] = 'sentiment' \n",
    "\n",
    "for i in range(0,model.get_num_topics()):\n",
    "    topic_keys.at[i,\"Topic\"] = i\n",
    "    topic_keys.at[i,\"Sentiment\"] = 'positive'\n",
    "x=0\n",
    "for words, scores, num in zip(topic_words, word_scores, topic_nums):\n",
    "    topic_keys.at[x,\"WordScore\"] = (words,scores)\n",
    "    x += 1\n",
    "    \n",
    "\n",
    "topic_keys\n",
    "\n",
    "topic_keys.to_csv(sentiment+'_'+camp+'_topicinfo.csv')\n",
    "\n",
    "# for words, scores, num in zip(topic_words, word_scores, topic_nums):\n",
    "#     print (f\"{words[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cef526-382c-4ded-96a7-6c0f58de21c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_num_topics(reduced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b85c3f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for topic in range(model.get_num_topics()):\n",
    "    model.generate_topic_wordcloud(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0c3fe3-fc95-45c4-875c-5ad593dfa279",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(fileName +\"_top2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459e6801",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_mapping = model.hierarchical_topic_reduction(num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079bbf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in range(model.get_num_topics(reduced=True)):\n",
    "    model.generate_topic_wordcloud(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff86cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_topic_hierarchy(reduced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a5343",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
